{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "import collections\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split # typically done at the start\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = pd.read_csv('./Parking_Violations_Issued_-_Fiscal_Year_2018.csv', usecols=  ['Summons Number', 'Registration State', 'Plate Type',\n",
    "       'Issue Date', 'Violation Code', 'Vehicle Body Type', 'Vehicle Make',\n",
    "       'Issuing Agency', 'Violation Time', 'Violation County', 'Street Name','Law Section',\n",
    "       'Sub Division', 'Vehicle Year', 'Feet From Curb'], index_col='Summons Number', nrows = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violations_df.columns\n",
    "# violations_df = violations_df.drop(columns=['Unregistered Vehicle?','Meter Number', 'Unregistered Vehicle?', 'Time First Observed','Intersecting Street','Double Parking Violation','No Standing or Stopping Violation', 'Hydrant Violation','Violation Post Code','Violation Legal Code', 'Intersecting Street','To Hours In Effect','From Hours In Effect','Violation Description','House Number', 'Violation In Front Of Or Opposite', 'Violation Location', 'Days Parking In Effect    '])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Registration State', 'Plate Type', 'Issue Date', 'Violation Code',\n",
       "       'Vehicle Body Type', 'Vehicle Make', 'Issuing Agency', 'Violation Time',\n",
       "       'Violation County', 'Street Name', 'Law Section', 'Sub Division',\n",
       "       'Vehicle Year', 'Feet From Curb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Summons Number', 'Registration State', 'Plate Type',\n",
    "       'Issue Date', 'Violation Code', 'Vehicle Body Type', 'Vehicle Make',\n",
    "       'Issuing Agency', 'Violation Time', 'Violation County', 'Street Name','Law Section',\n",
    "       'Sub Division', 'Vehicle Year', 'Feet From Curb']\n",
    "small_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_df = violations_df.sample(500000)\n",
    "small_df = small_df.dropna()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hours(x):\n",
    "   mid = int(len(x) / 2)\n",
    "   hrs = int(x[0:mid])\n",
    "   if (x[len(x) - 1] == 'P' or x[len(x) - 1] == 'A'):\n",
    "       mins = int(x[mid:len(x) - 1])\n",
    "       is_pm = x[len(x) - 1] == 'P'\n",
    "   else:\n",
    "       mins = int(x[mid:len(x)])\n",
    "       is_pm = False\n",
    "   if (is_pm):\n",
    "       hrs = hrs + 12\n",
    "   return hrs\n",
    "\n",
    "small_df['Violation Time'] = small_df['Violation Time'].dropna().apply(lambda x: convert_to_hours(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registration State            NY\n",
      "Plate Type                   PAS\n",
      "Issue Date            01/01/2018\n",
      "Violation Code                71\n",
      "Vehicle Body Type            SDN\n",
      "Vehicle Make               HYUND\n",
      "Issuing Agency                 P\n",
      "Violation Time                 2\n",
      "Violation County              NY\n",
      "Street Name           E 112TH ST\n",
      "Law Section                  408\n",
      "Sub Division                  D4\n",
      "Vehicle Year                   0\n",
      "Feet From Curb                 0\n",
      "Name: 1434219940, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(small_df.iloc[0])\n",
    "# small_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_df['Issue Date']= pd.to_datetime(small_df['Issue Date'])\n",
    "small_df.drop('Issue Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting type of parking violation\n",
    "Since we only have data about vehicles that have violated parking laws in NYC, the question we are now looking to answer is:  \n",
    "Can we predict the type of violation committed by a vehicle based on it's Registration State, Plate Type, Vehicle Body Type, Vehicle Make,Issuing Agency, Violation Time, Violation County, Street Name, Law Section, Sub Division, Vehicle Year and Feet From Curb. \n",
    "\n",
    "### Creating Dummy variables for categorical data\n",
    "The dataset we are using consists mostly of categorical data and conducting statistical analysis on categorical data is difficult and inefficent becasue it proposes various challenges and limitations. Text is hard to compare since there is no inherent structure or numeric attributes that define and rank some of the categories. Therefore we created dummy variables for each unique value in each category so it contains binary representation (either it belongs in that subcategory or not). The following kernels of code perform Hot Encoding of the categorical variables and joins them to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(small_df['Registration State'])\n",
    "one_hot.columns = ['Reg_State_' + str(col) for col in one_hot.columns]\n",
    "\n",
    "small_df.drop(columns=['Registration State'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_pl_type = pd.get_dummies(small_df['Plate Type'])\n",
    "one_hot_pl_type.columns = ['plate_type_' + str(col) for col in one_hot_pl_type.columns]\n",
    "\n",
    "small_df.drop(columns=['Plate Type'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_pl_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_v_body_type = pd.get_dummies(small_df['Vehicle Body Type'])\n",
    "one_hot_v_body_type.columns = ['v_body_type_' + str(col) for col in one_hot_v_body_type.columns]\n",
    "\n",
    "small_df.drop(columns=['Vehicle Body Type'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_v_body_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_v_make = pd.get_dummies(small_df['Vehicle Make'])\n",
    "one_hot_v_make.columns = ['v_make_' + str(col) for col in one_hot_v_make.columns]\n",
    "\n",
    "small_df.drop(columns=['Vehicle Make'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_v_make)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_issuing_agency = pd.get_dummies(small_df['Issuing Agency'])\n",
    "one_hot_issuing_agency.columns = ['issuing_agency_' + str(col) for col in one_hot_issuing_agency.columns]\n",
    "\n",
    "small_df.drop(columns=['Issuing Agency'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_issuing_agency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sub_div = pd.get_dummies(small_df['Sub Division'])\n",
    "one_hot_sub_div.columns = ['sub_div_' + str(col) for col in one_hot_sub_div.columns]\n",
    "\n",
    "small_df.drop(columns=['Sub Division'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_sub_div)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vio_cty = pd.get_dummies(small_df['Violation County'])\n",
    "one_hot_vio_cty.columns = ['vio_cty_' + str(col) for col in one_hot_vio_cty.columns]\n",
    "\n",
    "small_df.drop(columns=['Violation County'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_vio_cty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_street_name = pd.get_dummies(small_df['Street Name'])\n",
    "one_hot_street_name.columns = ['strt_name_' + str(col) for col in one_hot_street_name.columns]\n",
    "\n",
    "small_df.drop(columns=['Street Name'],inplace=True)\n",
    "\n",
    "small_df = small_df.join(one_hot_street_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "After encoding the data, we create decision trees based on all of our independent variables mentioned in the question above. \n",
    "We are using decision trees here because it is fast. We have a lot of data and a lost 5000 columns becasue of hot encoding the data. Most ML methods take an exponentially large time becasue of the size of the data set. Additionally, Decision trees are easy to interpret and visualize.It can easily capture Non-linear patterns. The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm. [Source](https://scikit-learn.org/stable/modules/tree.html)\n",
    "However, DTs also tend to overfit the data. Since we have so many columns and the categories are so varied, there is a lot of noise created, detracting us from accurately predicting type of parking violation.   \n",
    "  \n",
    "Although we only created the decision trees for a sample of the data, we noticed that as the data got bigger, the accuraccy score also got bigger. This may be becasue we are introducing additional columns becasue there might be new values of categories, adding to additional columns. However the updward trend suggests that a higher number of cases studied improves accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_outcome, test_outcome = train_test_split(\n",
    "   small_df.drop(columns=['Violation Code']),      # features\n",
    "   small_df['Violation Code'],    # outcome\n",
    "   test_size=0.30 # percentage of data to use as the test set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(train_features, train_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9187875514750556"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(tree_clf.predict(test_features), test_outcome)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN \n",
    "K Nearest neighbors takes an exceptionally long time to run since it has to cross validate the data and perform a grid search to find the best value of K. We were unable to compile the code becasue of memory errors and time it took to run the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1,20), 'kneighborsclassifier__weights':[\"uniform\", \"distance\"]}\n",
    "grid = GridSearchCV(pipe)\n",
    "grid.fit(train_features, train_outcome)\n",
    "grid.score(test_features, test_outcome)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
